{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.functional import pad\n",
    "from transformers import RobertaTokenizerFast, RobertaConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv to txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part also the dates are being removed to avoid the model learning pattern by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19260\\3859717732.py:3: FutureWarning: using <function <lambda> at 0x000001F60608D8A0> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
      "  df.Time = df.Time.agg(lambda x: x.split(' ')[1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.\\\\data\\\\btc-usdt.csv', index_col = False)\n",
    "df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "df.Time = df.Time.agg(lambda x: x.split(' ')[1])\n",
    "df.to_csv(f'{os.getcwd()}\\\\data\\\\btc-usdt-mod.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV has been successfully converted to a text file btc-usdt-mod.txt\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file and the text file\n",
    "with open('.\\\\data\\\\btc-usdt-mod.csv', mode='r') as csv_file, open('.\\\\data\\\\btc-usdt-mod.txt', mode='w') as txt_file:\n",
    "    # Create a CSV reader\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)\n",
    "    # Loop through each row in the CSV\n",
    "    for row in csv_reader:        \n",
    "        # Join the row with commas and a space, then write it to the text file with a newline\n",
    "        txt_file.write(' '.join(row) + '\\n')\n",
    "\n",
    "print('CSV has been successfully converted to a text file', 'btc-usdt-mod.txt')\n",
    "data_path = '.\\\\data\\\\btc-usdt-mod.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\tokenizer\\\\btc-vocab.json', '.\\\\tokenizer\\\\btc-merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=data_path, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "try:\n",
    "    os.mkdir('tokenizer')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\".\\\\tokenizer\", \"btc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, k):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - k):\n",
    "        sequence = \" <s> \" + \" \".join(data[i:i+k]) + \" </s>\"\n",
    "        yield sequence\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "with open('.\\\\data\\\\btc-test.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Example: create sequences with k=3\n",
    "k = 3\n",
    "sequences = create_sequences(lines, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into training (1678209 lines) and testing (419553 lines) sets.\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "split_index = int(len(lines) * 0.8)\n",
    "\n",
    "train_lines = lines[:split_index]\n",
    "test_lines = lines[split_index:]\n",
    "\n",
    "with open('.\\\\data\\\\btc-train.txt', 'w') as file:\n",
    "    file.writelines(train_lines)\n",
    "    \n",
    "with open('.\\\\data\\\\btc-test.txt', 'w') as file:\n",
    "    file.writelines(test_lines)\n",
    "\n",
    "print(f'Data has been split into training ({len(train_lines)} lines) and testing ({len(test_lines)} lines) sets.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTCDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False, seq_len=11):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \".\\\\tokenizer\\\\btc-vocab.json\",\n",
    "            \".\\\\tokenizer\\\\btc-merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "        self.sequences = []\n",
    "\n",
    "        src_files = Path(\".\\\\data\").glob(\"btc-test.txt\") if evaluate else Path(\".\\\\data\").glob(\"btc-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"processing\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.sequences +=  [x.ids for x in tokenizer.encode_batch(create_sequences(lines, seq_len))]\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.sequences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.embedding(x) *  math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Wise Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, drop_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lookahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_mask(shape):\n",
    "    mask = torch.ones(shape, shape).tril()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ffn: int,\n",
    "        drop_p: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model,d_ffn)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ffn,d_model)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerAddNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "        self.b = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sub_x: torch.Tensor):\n",
    "        try:\n",
    "            add = x + sub_x\n",
    "            mean = add.mean(-1, keepdim=True)\n",
    "            std = add.std(-1, keepdim=True)\n",
    "            return self.g*(add - mean)/(std+self.eps) + self.b\n",
    "        except RuntimeError as e:\n",
    "            error_message = str(e)\n",
    "            if \"size of tensor x\" in error_message and \"must match the size of tensor sub_x\" in error_message:\n",
    "                print(\"Caught a tensor size mismatch error in sublayer addition.\")\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    q: torch.Tensor, \n",
    "    k: torch.Tensor,\n",
    "    v: torch.Tensor, \n",
    "    mask: Optional[torch.Tensor] = None, # 1, 1, attn_dim, attn_dim\n",
    "    drop_p: float = 0.1\n",
    "):\n",
    "    d_k = q.size(-1)\n",
    "    \n",
    "    att_score = torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        att_score = att_score.masked_fill(mask == 0, float('-inf'))\n",
    "    att_prob = F.softmax(att_score, dim = -1)\n",
    "    \n",
    "    dropout = nn.Dropout(drop_p)\n",
    "    attn_p = dropout(att_prob)\n",
    "    \n",
    "    scores = torch.matmul(attn_p,v)\n",
    "    \n",
    "    return scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_h: int,\n",
    "        drop_p: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = None\n",
    "\n",
    "        self.d_k = d_model // num_h\n",
    "        self.num_h = num_h\n",
    "        self.proj_q = nn.Linear(self.d_k, self.d_k)\n",
    "        self.proj_k = nn.Linear(self.d_k, self.d_k)\n",
    "        self.proj_v = nn.Linear(self.d_k, self.d_k)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "    ):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "         \n",
    "        bs, seq_len, _ = query.size()\n",
    "\n",
    "        query = query.view(bs, -1, self.num_h, self.d_k).transpose(1,2)\n",
    "        key = key.view(bs, -1, self.num_h, self.d_k).transpose(1,2)\n",
    "        value = value.view(bs, -1, self.num_h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        query = self.proj_q(query)\n",
    "        key = self.proj_k(key)\n",
    "        value = self.proj_v(value)\n",
    "        \n",
    "        scores = scaled_dot_product_attention(query, key ,value, mask = mask, drop_p = self.drop_p)\n",
    "        scores = scores.transpose(-2,-3).reshape(bs, seq_len, self.d_k * self.num_h)\n",
    "        out = self.out_proj(scores)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.addnorm1 = LayerAddNorm(embed_dim) \n",
    "        self.addnorm2 = LayerAddNorm(embed_dim) \n",
    "        self.feedfwd = FeedForward(embed_dim, expansion_factor*embed_dim)\n",
    "\n",
    "    def forward(self,key,query,value):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        att_out = self.attention(key,query,value) \n",
    "        addnorm1_out = self.addnorm1(att_out,value)\n",
    "\n",
    "        feed_fwd_out = self.feedfwd(addnorm1_out)\n",
    "        addnorm2_out = self.addnorm2(feed_fwd_out, addnorm1_out)\n",
    "\n",
    "        return addnorm2_out\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PosEmbedding(embed_dim, max_len=seq_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out,out,out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m enc \u001b[38;5;241m=\u001b[39m TransformerEncoder(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m,(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\AUA\\Generative AI\\GenAI_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\AUA\\Generative AI\\GenAI_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 59\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     58\u001b[0m     embed_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x)\n\u001b[1;32m---> 59\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     61\u001b[0m         out \u001b[38;5;241m=\u001b[39m layer(out,out,out)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\AUA\\Generative AI\\GenAI_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\AUA\\Generative AI\\GenAI_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mPosEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "enc = TransformerEncoder(10,512,6,4,8)\n",
    "x = torch.randint(1,10,(1000,32))\n",
    "out = enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.addnorm = nn.LayerAddNorm(embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "        \n",
    "    \n",
    "    def forward(self, key, query, x,mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #we need to pass mask mask only to fst attention\n",
    "        att_out = self.attention(x,x,x,mask=mask) #32x10x512\n",
    "        addnorm_out = self.addnorm(att_out,x)\n",
    "        \n",
    "        out = self.transformer_block(key, query, addnorm_out)\n",
    "\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = TokenEmbedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PosEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        x = self.word_embedding(x)  #32x10x512\n",
    "        x = self.position_embedding(x) #32x10x512\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        out = F.softmax(self.fc_out(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        \"\"\"\n",
    "        for inference\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out_labels : returns final prediction of sequence\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        out = trg\n",
    "        for i in range(seq_len): #10\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            # taking the last token\n",
    "            out = out[:,-1,:]\n",
    "     \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "          \n",
    "        \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "   \n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI_venv",
   "language": "python",
   "name": "genai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
